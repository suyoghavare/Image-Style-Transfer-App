{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13836183,"sourceType":"datasetVersion","datasetId":8811984}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport glob\n\n\nclass VideoStyleTransfer:\n    def __init__(self, device=None):\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.device = torch.device(device)\n        print(f\"Using device: {self.device}\")\n\n        try:\n            weights = models.VGG19_Weights.DEFAULT\n            vgg = models.vgg19(weights=weights)\n        except TypeError:\n            vgg = models.vgg19(pretrained=True)\n\n        self.vgg = vgg.features.to(self.device).eval()\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n\n        self.imagenet_mean = [0.485, 0.456, 0.406]\n        self.imagenet_std = [0.229, 0.224, 0.225]\n\n        self.mean = torch.tensor(self.imagenet_mean).view(1, 3, 1, 1).to(self.device)\n        self.std = torch.tensor(self.imagenet_std).view(1, 3, 1, 1).to(self.device)\n\n        self.content_idx = 21\n        self.style_indices = [0, 5, 10, 19, 28]\n\n    def _transform(self):\n        return transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(self.imagenet_mean, self.imagenet_std),\n        ])\n\n    def load_image(self, path):\n        img = Image.open(path).convert(\"RGB\")\n        transform = self._transform()\n        tensor = transform(img).unsqueeze(0).to(self.device)\n        return tensor, img.size\n\n    def denormalize(self, tensor):\n        result = tensor.detach().clone()\n        result = result * self.std + self.mean\n        result = torch.clamp(result, 0, 1)\n        return result.cpu()\n\n    def get_feature_maps(self, x):\n        features = []\n        for i, layer in enumerate(self.vgg):\n            x = layer(x)\n            if i == self.content_idx or i in self.style_indices:\n                features.append((i, x))\n        return features\n\n    def gram_matrix(self, x):\n        batch_size, channels, height, width = x.size()\n        features = x.view(batch_size, channels, height * width)\n        gram = torch.bmm(features, features.transpose(1, 2))\n        return gram / (height * width)\n\n    def total_variation(self, img):\n        horizontal_diff = torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:])\n        vertical_diff = torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :])\n        return horizontal_diff.mean() + vertical_diff.mean()\n\n    def extract_frames_from_video(self, video_path, output_folder, frame_skip=1):\n        print(f\"Extracting frames from: {video_path}\")\n        \n        os.makedirs(output_folder, exist_ok=True)\n        \n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Unable to open video: {video_path}\")\n        \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        print(f\"Video details: {total_frames} frames, {width}x{height}, {fps:.2f} FPS\")\n        print(f\"Frame skip: {frame_skip}\")\n        \n        saved_count = 0\n        \n        for frame_idx in tqdm(range(0, total_frames, frame_skip)):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_path = os.path.join(output_folder, f\"frame_{saved_count:06d}.jpg\")\n            cv2.imwrite(frame_path, frame)\n            saved_count += 1\n        \n        cap.release()\n        print(f\"Extracted {saved_count} frames to {output_folder}\")\n        return saved_count, fps, (width, height)\n\n    def style_transfer_single_frame(\n        self,\n        content_path,\n        style_path,\n        output_path,\n        content_weight=1e5,\n        style_weight=3e4,\n        tv_weight=1.0,\n        optimizer_type=\"lbfgs\",\n        num_steps_lbfgs=100,\n        lr_adam=10.0,\n    ):\n        content_img, content_size = self.load_image(content_path)\n        style_img, _ = self.load_image(style_path)\n\n        print(f\"Processing frame: {content_size[0]}x{content_size[1]}\")\n\n        optimizing_img = content_img.clone().requires_grad_(True)\n\n        with torch.no_grad():\n            content_feats = self.get_feature_maps(content_img)\n            style_feats = self.get_feature_maps(style_img)\n\n        target_content = None\n        for idx, feat in content_feats:\n            if idx == self.content_idx:\n                target_content = feat.squeeze(0)\n                break\n\n        target_style_grams = []\n        for idx, feat in style_feats:\n            if idx in self.style_indices:\n                gram = self.gram_matrix(feat)\n                target_style_grams.append(gram)\n\n        if optimizer_type == \"adam\":\n            optimizer = optim.Adam([optimizing_img], lr=lr_adam)\n            max_iter = num_steps_lbfgs\n        else:\n            optimizer = optim.LBFGS([optimizing_img], max_iter=num_steps_lbfgs)\n            max_iter = num_steps_lbfgs\n\n        def compute_losses(img):\n            feats = self.get_feature_maps(img)\n\n            current_content = None\n            style_feats_current = []\n            for idx, feat in feats:\n                if idx == self.content_idx:\n                    current_content = feat.squeeze(0)\n                if idx in self.style_indices:\n                    style_feats_current.append(feat)\n\n            content_loss = nn.MSELoss(reduction=\"mean\")(current_content, target_content)\n\n            style_loss = 0.0\n            current_grams = [self.gram_matrix(f) for f in style_feats_current]\n            for target_gram, current_gram in zip(target_style_grams, current_grams):\n                style_loss += nn.MSELoss(reduction=\"sum\")(target_gram, current_gram)\n            style_loss /= len(target_style_grams)\n\n            tv_loss = self.total_variation(img)\n\n            total_loss = (\n                content_weight * content_loss\n                + style_weight * style_loss\n                + tv_weight * tv_loss\n            )\n            return total_loss\n\n        if optimizer_type == \"adam\":\n            for step in range(max_iter):\n                optimizer.zero_grad()\n                total_loss = compute_losses(optimizing_img)\n                total_loss.backward()\n                optimizer.step()\n                \n                if step % 20 == 0:\n                    print(f\"Step {step}/{max_iter}, Loss: {total_loss.item():.4f}\")\n        else:\n            step_counter = [0]\n            def closure():\n                optimizer.zero_grad()\n                total_loss = compute_losses(optimizing_img)\n                total_loss.backward()\n                if step_counter[0] % 50 == 0:\n                    print(f\"Step {step_counter[0]}, Loss: {total_loss.item():.4f}\")\n                step_counter[0] += 1\n                return total_loss\n            optimizer.step(closure)\n\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        final_img = self.denormalize(optimizing_img)\n        save_image(final_img, output_path)\n        \n        print(f\"Saved: {os.path.basename(output_path)}\")\n        return optimizing_img\n\n    def process_all_frames(\n        self,\n        frames_folder,\n        style_path,\n        output_folder,\n        content_weight=1e5,\n        style_weight=3e4,\n        tv_weight=1.0,\n        optimizer_type=\"lbfgs\",\n        num_steps_lbfgs=50,\n        lr_adam=10.0,\n        frame_skip=1\n    ):\n        print(f\"Processing frames from: {frames_folder}\")\n        \n        os.makedirs(output_folder, exist_ok=True)\n        \n        frame_files = sorted(glob.glob(os.path.join(frames_folder, \"*.jpg\")))\n        if not frame_files:\n            frame_files = sorted(glob.glob(os.path.join(frames_folder, \"*.png\")))\n        \n        if not frame_files:\n            raise ValueError(f\"No image files found in {frames_folder}\")\n        \n        frame_files = frame_files[::frame_skip]\n        \n        print(f\"Found {len(frame_files)} frames to process\")\n        \n        successful_frames = 0\n        \n        for i, frame_file in enumerate(tqdm(frame_files, desc=\"Processing frames\")):\n            frame_name = os.path.basename(frame_file)\n            output_path = os.path.join(output_folder, frame_name)\n            \n            if os.path.exists(output_path):\n                print(f\"Skipping already processed: {frame_name}\")\n                successful_frames += 1\n                continue\n                \n            try:\n                print(f\"Processing frame {i+1}/{len(frame_files)}: {frame_name}\")\n                self.style_transfer_single_frame(\n                    content_path=frame_file,\n                    style_path=style_path,\n                    output_path=output_path,\n                    content_weight=content_weight,\n                    style_weight=style_weight,\n                    tv_weight=tv_weight,\n                    optimizer_type=optimizer_type,\n                    num_steps_lbfgs=num_steps_lbfgs,\n                    lr_adam=lr_adam,\n                )\n                successful_frames += 1\n                \n            except Exception as e:\n                print(f\"Error processing {frame_file}: {e}\")\n                continue\n        \n        print(f\"Processed {successful_frames}/{len(frame_files)} frames to {output_folder}\")\n        return successful_frames\n\n    def create_video_from_frames(\n        self,\n        frames_folder,\n        output_video_path,\n        fps=30,\n        frame_size=None\n    ):\n        print(f\"Creating video from frames in: {frames_folder}\")\n        \n        frame_files = sorted(glob.glob(os.path.join(frames_folder, \"*.jpg\")))\n        if not frame_files:\n            frame_files = sorted(glob.glob(os.path.join(frames_folder, \"*.png\")))\n        \n        if not frame_files:\n            raise ValueError(f\"No image files found in {frames_folder}\")\n        \n        print(f\"Found {len(frame_files)} frames for video creation\")\n        \n        first_frame = cv2.imread(frame_files[0])\n        if first_frame is None:\n            raise ValueError(f\"Unable to read first frame: {frame_files[0]}\")\n        \n        if frame_size is None:\n            frame_size = (first_frame.shape[1], first_frame.shape[0])\n        \n        print(f\"Video resolution: {frame_size[0]}x{frame_size[1]} at {fps} FPS\")\n        \n        codec_configs = [\n            ('XVID', '.avi'),\n            ('MJPG', '.avi'),\n            ('mp4v', '.mp4'),\n        ]\n        \n        video_writer = None\n        final_output_path = output_video_path\n        \n        for codec, ext in codec_configs:\n            try:\n                temp_path = output_video_path.rsplit('.', 1)[0] + ext\n                fourcc = cv2.VideoWriter_fourcc(*codec)\n                video_writer = cv2.VideoWriter(temp_path, fourcc, fps, frame_size)\n                \n                if video_writer.isOpened():\n                    print(f\"Using codec: {codec}\")\n                    final_output_path = temp_path\n                    break\n                else:\n                    if video_writer:\n                        video_writer.release()\n                    video_writer = None\n            except Exception as e:\n                print(f\"Failed with {codec}: {e}\")\n                continue\n        \n        if video_writer is None:\n            video_writer = cv2.VideoWriter(final_output_path, 0, fps, frame_size)\n            if not video_writer.isOpened():\n                raise RuntimeError(\"Unable to create video writer\")\n\n        for frame_file in tqdm(frame_files, desc=\"Creating video\"):\n            frame = cv2.imread(frame_file)\n            if frame is None:\n                print(f\"Unable to read frame: {frame_file}\")\n                continue\n            \n            if (frame.shape[1], frame.shape[0]) != frame_size:\n                frame = cv2.resize(frame, frame_size)\n            \n            video_writer.write(frame)\n        \n        video_writer.release()\n        print(f\"Video created: {final_output_path}\")\n        print(f\"Resolution: {frame_size[0]}x{frame_size[1]}\")\n        print(f\"FPS: {fps}\")\n        print(f\"Total frames: {len(frame_files)}\")\n        \n        return final_output_path\n\n    def process_video(\n        self,\n        video_path,\n        style_path,\n        output_video_name=\"video-final-output\",\n        content_weight=1e5,\n        style_weight=3e4,\n        tv_weight=1.0,\n        optimizer_type=\"lbfgs\",\n        num_steps_lbfgs=50,\n        lr_adam=10.0,\n        frame_skip=1,\n        fps=None\n    ):\n        frames_folder = \"/kaggle/working/video-frames\"\n        output_frames_folder = \"/kaggle/working/video-frame-outputs\"\n        output_video_path = f\"/kaggle/working/{output_video_name}.mp4\"\n        \n        print(\"Starting video style transfer pipeline\")\n        print(f\"Input video: {video_path}\")\n        print(f\"Style image: {style_path}\")\n        print(f\"Output video: {output_video_path}\")\n        print(f\"Frame skip: {frame_skip}\")\n        \n        print(\"Step 1: Extracting frames from video\")\n        frame_count, original_fps, original_size = self.extract_frames_from_video(\n            video_path=video_path,\n            output_folder=frames_folder,\n            frame_skip=frame_skip\n        )\n        \n        print(f\"Original video size: {original_size[0]}x{original_size[1]}\")\n        \n        if fps is None:\n            fps = original_fps / frame_skip\n            print(f\"Using FPS: {fps:.2f}\")\n        \n        print(\"Step 2: Applying style transfer to frames\")\n        processed_count = self.process_all_frames(\n            frames_folder=frames_folder,\n            style_path=style_path,\n            output_folder=output_frames_folder,\n            content_weight=content_weight,\n            style_weight=style_weight,\n            tv_weight=tv_weight,\n            optimizer_type=optimizer_type,\n            num_steps_lbfgs=num_steps_lbfgs,\n            lr_adam=lr_adam,\n            frame_skip=1\n        )\n        \n        print(\"Step 3: Creating video from processed frames\")\n        final_video_path = self.create_video_from_frames(\n            frames_folder=output_frames_folder,\n            output_video_path=output_video_path,\n            fps=fps,\n            frame_size=original_size\n        )\n        \n        print(\"Pipeline completed\")\n        print(f\"Extracted frames: {frame_count}\")\n        print(f\"Processed frames: {processed_count}\")\n        print(f\"Original resolution: {original_size[0]}x{original_size[1]}\")\n        print(f\"Final video: {final_video_path}\")\n        \n        return final_video_path\n\n\nif __name__ == \"__main__\":\n    processor = VideoStyleTransfer()\n    \n    VIDEO_PATH = \"/kaggle/input/finalvideo/V1.mp4\"\n    STYLE_PATH = \"/kaggle/input/finalvideo/sea-landscape-with-digital-art-style (1).jpg\"\n    \n    print(\"Starting Video Style Transfer\")\n    print(\"=\" * 50)\n    \n    try:\n        final_video = processor.process_video(\n            video_path=VIDEO_PATH,\n            style_path=STYLE_PATH,\n            output_video_name=\"video-final-output\",\n            content_weight=1e5,\n            style_weight=3e4,\n            tv_weight=1.0,\n            optimizer_type=\"lbfgs\",\n            num_steps_lbfgs=300,\n            frame_skip=1,\n            fps=30\n        )\n        \n        print(f\"Success: {final_video}\")\n        \n        output_frames_folder = \"/kaggle/working/video-frame-outputs\"\n        frame_files = sorted(glob.glob(os.path.join(output_frames_folder, \"*.jpg\")))[:3]\n        \n        if frame_files:\n            fig, axes = plt.subplots(1, min(3, len(frame_files)), figsize=(15, 5))\n            if len(frame_files) == 1:\n                axes = [axes]\n            \n            for i, frame_file in enumerate(frame_files):\n                img = Image.open(frame_file)\n                axes[i].imshow(img)\n                axes[i].set_title(f\"Frame {i+1} - {img.size[0]}x{img.size[1]}\")\n                axes[i].axis('off')\n            \n            plt.tight_layout()\n            plt.show()\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n    print(\"Disk usage summary:\")\n    frames_size = sum(os.path.getsize(os.path.join(\"/kaggle/working/video-frames\", f)) \n                     for f in os.listdir(\"/kaggle/working/video-frames\") \n                     if os.path.isfile(os.path.join(\"/kaggle/working/video-frames\", f))) / (1024*1024)\n    output_size = sum(os.path.getsize(os.path.join(\"/kaggle/working/video-frame-outputs\", f)) \n                     for f in os.listdir(\"/kaggle/working/video-frame-outputs\") \n                     if os.path.isfile(os.path.join(\"/kaggle/working/video-frame-outputs\", f))) / (1024*1024)\n    print(f\"Input frames: {frames_size:.1f} MB\")\n    print(f\"Output frames: {output_size:.1f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T08:05:40.425651Z","iopub.execute_input":"2025-11-23T08:05:40.426087Z","execution_failed":"2025-11-23T08:08:20.409Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 211MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Starting Video Style Transfer\n==================================================\nStarting video style transfer pipeline\nInput video: /kaggle/input/finalvideo/V1.mp4\nStyle image: /kaggle/input/finalvideo/sea-landscape-with-digital-art-style (1).jpg\nOutput video: /kaggle/working/video-final-output.mp4\nFrame skip: 1\nStep 1: Extracting frames from video\nExtracting frames from: /kaggle/input/finalvideo/V1.mp4\nVideo details: 73 frames, 720x1280, 30.10 FPS\nFrame skip: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 73/73 [00:03<00:00, 22.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracted 73 frames to /kaggle/working/video-frames\nOriginal video size: 720x1280\nStep 2: Applying style transfer to frames\nProcessing frames from: /kaggle/working/video-frames\nFound 73 frames to process\n","output_type":"stream"},{"name":"stderr","text":"Processing frames:   0%|          | 0/73 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Processing frame 1/73: frame_000000.jpg\nProcessing frame: 720x1280\nStep 0, Loss: 2953971712.0000\nStep 50, Loss: 35346464.0000\nStep 100, Loss: 13362424.0000\nStep 150, Loss: 8298064.5000\nStep 200, Loss: 6155570.5000\nStep 250, Loss: 4971092.5000\n","output_type":"stream"},{"name":"stderr","text":"Processing frames:   1%|▏         | 1/73 [01:59<2:23:25, 119.53s/it]","output_type":"stream"},{"name":"stdout","text":"Saved: frame_000000.jpg\nProcessing frame 2/73: frame_000001.jpg\nProcessing frame: 720x1280\nStep 0, Loss: 2934895104.0000\nStep 50, Loss: 36913260.0000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}